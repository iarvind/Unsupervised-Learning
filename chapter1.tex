\chapter{Backpropagation}
% Define: 
%  - Cost / objective function
%  - Gradient descent to minimise cost by finding set of weights and biases.
%  - Backpropagation
%  - Stochastic Gradient descent
 
We use MNIST handwritten data as a training data set. We define our Objective / Cost function and minimise it by finding set of weights and biases using Gradient Descent. Next, we define Backpropagation algorithm which can efficiently compute gradients.

\section{Learning with Gradient Descent}
We need algorithm which lets us find weights and biases so that the output from the network approximates y(x) for all training inputs x. 
To determine how well we're achieving this goal, we define a Cost / Objective function:\\
\begin{eqnarray}  C(w,b) \equiv
\frac{1}{2n} \sum_x \| y(x) - a\|^2.
\end{eqnarray}
- w is collection of all weights in the network\\
- b is collection of all biases\\
- n is total training inputs\\
- a is the vector of outputs from network when x is input.\\

\section{Gradient Descent}
To minimize cost by finding set of weights and biases, we use Gradient Descent.
Let's suppose we're trying to minimize some function, C(v) with many variables v = v1, v2, v3,... For small $\Delta v_{2}$ in $v_{2}$ direction and $\Delta v_{1}$ in $v_{1}$ direction, we get:
\begin{eqnarray} 
\Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
\frac{\partial C}{\partial v_2} \Delta v_2.
\end{eqnarray}
To make $\Delta C$ negative, we define \\
$\Delta v$ as vector of changes in v,  
$\Delta v \equiv (\Delta v_1, \Delta v_2)^T$ and  \\
gradient of C as vector of partial derivates,

\begin{eqnarray} 
\nabla C \equiv \left( \frac{\partial C}{\partial v_1}, 
\frac{\partial C}{\partial v_2} \right)^T.
\end{eqnarray}
$\Delta C$ can be rewritten as
\begin{eqnarray} 
\Delta C \approx \nabla C \cdot \Delta v.
\end{eqnarray}
This equation basically helps us in selecting $\Delta v$ so that we can find negative $\Delta C$.
Suppose we choose $\Delta v$ as:
\begin{eqnarray} 
\Delta v = -\eta \nabla C
\end{eqnarray}
where $\eta$ is a small positive parameter (learning rate).
Above equation tells us that 
$\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta \|\nabla C\|^2$
This guarantees that $\Delta C \leq 0$, i.e., C will always decrease, never increase.Hence, 
\begin{eqnarray}
v \rightarrow v' = v -\eta \nabla C.
\end{eqnarray}

\subsection{Gradient Descent and Neural Network}
We apply same gradient descent update rule in terms of components $w_{k}$ and $b_{l}$, we have
\begin{eqnarray}
w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k}\\
b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}
\end{eqnarray}

\subsection{Stochastic Gradient Descent}
The cost function has the form $C = \frac{1}{n} \sum_x C_x$ and averages over costs $C_x \equiv \frac{\|y(x)-a\|^2}{2}$ for every training example. In practice, we need to calculate gradients $\nabla C_x$ separately for each training example, $x$, and the take average. With the increase in training examples, this learning occurs slowly and there, we use Stochastic Gradient Descent.\\
To estimate $\nabla C$, we randomly choose training inputs and calculate $\nabla C_x$. It turns out to be a good estimate of true gradient $\nabla C$ and helps speed up gradient descent.\\
For small number $m$ of randomly chosen training inputs, we label them as  $X_1, X_2,...,X_m$  and call them mini-batch. The average value of $\nabla C_{X_{j}}$ estimates the average over all $\nabla C_x$, i.e.,
\begin{eqnarray}
\frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
\end{eqnarray}
or,
\begin{eqnarray}
\nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}},
\end{eqnarray}

Suppose $w_k$ and $b_l$ denote the weights and biases in our neural network. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those,
\begin{eqnarray} 
w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m}
\sum_j \frac{\partial C_{X_j}}{\partial w_k}\\
b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m}
\sum_j \frac{\partial C_{X_j}}{\partial b_l},
\end{eqnarray}

\section{BackPropagation}
Backpropagation is a general method to train artificial neural networks which is used along with gradient descent.
Notations in feedforward neural network:
$w^l_{jk}$ :  weight from the $k^{th}$ neuron in $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\\
$b^l_j$ : bias of the $j^{th}$ neuron in the $l^{th}$ layer.\\
$a^l_j$ : activation of the $j^{th}$ neuron in the $l^{th}$ layer.\\
$z^l$ : weighted input.\\

We can relate activation of $j^{th}$ neuron in $l^{th}$ layer with $(l-1)^{th}$ layer as:
\begin{eqnarray} 
a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),
\end{eqnarray}
or in vectorized form:
\begin{eqnarray} 
a^{l} = \sigma(w^l a^{l-1}+b^l) = \sigma(z^l)
\end{eqnarray}
We define weighted input as:
$z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j$ \\
or in vectorized compact form: 
$z^l = w^l a^{l-1}+b^l$\\

\subsection{Assumptions about cost function}

\begin{eqnarray}
C = \frac{1}{2n} \sum_x \|y(x)-a^L(x)\|^2,
\end{eqnarray}
\textbf{Assumption 1:} Cost function C can be written as an average $C = \frac{1}{n} \sum_x C_x$ over cost functions $C_x$ for training examples, x.\\
\textbf{Assumption 2:} Cost can be written as a function of the outputs from the neural network.\\
Quadratic cost function satisfies as:
\begin{eqnarray}
C = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2,
\end{eqnarray}

\subsection{Fundamental equations}
\textbf{Notation:}\\
$\delta^l_j$ : Error in the $j^{th}$ neuron in the $l^{th}$ layer.
\begin{eqnarray} 
\delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.
\end{eqnarray}
\subsubsection{Equations}
\begin{eqnarray} 
\delta^L = \nabla_a C \odot \sigma'(z^L)\\
\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)\\
\frac{\partial C}{\partial b^l_j} = \delta^l_j\\
\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j
\end{eqnarray}

\subsubsection{Algorithm steps}
\begin{enumerate}
\item \textbf{Input x}: Set the activation $a^1$
\item \textbf{Feedforward}: For l = 2,3,4,...,L, compute $z^l$ and $a^l$
\item \textbf{Output error $\delta^L$}: Compute vector $\delta^L$
\item \textbf{Backpropagate the error}: For each l = L-1, L-2,...,2, compute $\delta^l$
\item \textbf{Output}: The gradient of the cost is gives by $\frac{\partial C}{\partial b^l_j}$ and $\frac{\partial C}{\partial w^l_{jk}}$.
\end{enumerate}

\section{Implementation}
We implement neural network to recognize handwritten digits. We use MNIST data set, which contains images of handwritten digits together with classification of digits. This training data consists of 28 by 28 grayscale images, so we set input layer to 784 neurons = 28 x 28. The pixel value varies between 0.0 to 1.0. We have 10 output neurons representing digits from 0 to 9 (all set to zero except one). The value is selected upon highest activation value.\\
\textbf{Input}: x as 784 dimensional vector.\\
\textbf{Output}: y = y(x) as 10 dimensional vector.\\
With Stochastic gradient descent, we randomly choose mini-batch of training examples. After, we pick another randomly chosen mini-batch, we continue until we exhaust the training inputs. This is called completing an \textit{epoch} of training.\\
We further test our implementation with test images and find classification rate.
%Epoch 4: 9434 / 10000
%Epoch 5: 9535 / 10000
%Epoch 6: 9524 / 10000
%Epoch 7: 9535 / 10000
%Epoch 8: 9532 / 10000
%Epoch 9: 9533 / 10000
%Epoch 10: 9542 / 10000
%Epoch 11: 9600 / 10000
%Epoch 12: 9549 / 10000
%Epoch 13: 9584 / 10000
%Epoch 14: 9574 / 10000
%Epoch 15: 9604 / 10000
\texttt{setup:\\
$\eta$: 3.0\\
layers: 784,50,50,10 \\
epoch: 20\\ 
mini-batch size: 10
}

\begin{lstlisting}
>>> net = network.Network([784,50,50,10])
>>> SGD(training_data, 20, 10, 3.0, test_data=test_data)
Epoch 0: 9083 / 10000
Epoch 1: 9315 / 10000
Epoch 2: 9346 / 10000
Epoch 3: 9423 / 10000
.
.
.
Epoch 17: 9593 / 10000
Epoch 18: 9585 / 10000
Epoch 19: 9594 / 10000
\end{lstlisting}

We see that trained network gives us classification rate of about 96 percent.